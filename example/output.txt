-----------  Configuration Arguments -----------
backend: auto
cluster_topo_path: None
coordinator_num: None
coordinators: 
elastic_pre_hook: None
elastic_server: None
enable_auto_mapping: False
force: False
gpus: 2,3
heter_devices: 
heter_worker_num: None
heter_workers: 
host: None
http_port: None
ips: 127.0.0.1
job_id: None
log_dir: log
np: None
nproc_per_node: None
rank_mapping_path: None
run_mode: None
scale: 0
server_num: None
servers: 
training_script: train_by_paddle_multi.py
training_script_args: []
worker_num: None
workers: 
------------------------------------------------
launch train in GPU mode!
launch proc_id:3167673 idx:0
launch proc_id:3167675 idx:1
[2025-05-09 17:17:30,983] [    INFO] distributed_strategy.py:214 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
=======================================================================
I0509 17:17:30.984509 3167673 tcp_utils.cc:181] The server starts to listen on IP_ANY:49929
I0509 17:17:30.984658 3167673 tcp_utils.cc:130] Successfully connected to 127.0.0.1:49929
I0509 17:17:34.049408 3167673 process_group_nccl.cc:129] ProcessGroupNCCL pg_timeout_ 1800000
[2025-05-09 17:17:34,050] [    INFO] topology.py:358 - Total 2 pipe comm group(s) create successfully!
W0509 17:17:34.067167 3167673 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W0509 17:17:34.126135 3167673 gpu_resources.cc:164] device: 2, cuDNN Version: 8.7.
/home/jiyiming/.local/lib/python3.10/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 0 is not in group _default_pg10
  warnings.warn(
I0509 17:17:35.056531 3167673 process_group_nccl.cc:129] ProcessGroupNCCL pg_timeout_ 1800000
[2025-05-09 17:17:35,056] [    INFO] topology.py:358 - Total 1 data comm group(s) create successfully!
[2025-05-09 17:17:35,056] [    INFO] topology.py:358 - Total 2 model comm group(s) create successfully!
[2025-05-09 17:17:35,056] [    INFO] topology.py:358 - Total 2 sharding comm group(s) create successfully!
I0509 17:17:35.056886 3167673 process_group_nccl.cc:129] ProcessGroupNCCL pg_timeout_ 1800000
[2025-05-09 17:17:35,056] [    INFO] topology.py:288 - HybridParallelInfo: rank_id: 0, mp_degree: 1, sharding_degree: 1, pp_degree: 1, dp_degree: 2, sep_degree: 1, mp_group: [0],  sharding_group: [0], pp_group: [0], dp_group: [0, 1], sep:group: None, check/clip group: [0]
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
=======================================================================
FileStore path: ./tmp/bmtrain_store_localhost_10010
world_size:2 pp_size:1 tp_size:1
len(all_available_cpus) 128, local_size:2
local_rank: 0, cpus_this_worker: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
FileStore set ./tmp/bmtrain_store_localhost_10010/BMTRAIN_UNIQUE_ID.tmp 4f199d15c95d8a350200c94dc0a8a2c30000000000000000000000000000000000000000000000004f690156d67f0000d0edf4ccd05500009ce46ad7d47f0000a0214858fc7f000020000000000000000d000000000000006063a83fd1550000000000000000000080ffffffffffffffa0214858fc7f00000000000000000000
rank 0 unique_id 4f199d15c95d8a350200c94dc0a8a2c30000000000000000000000000000000000000000000000004f690156d67f0000d0edf4ccd05500009ce46ad7d47f0000a0214858fc7f000020000000000000000d000000000000006063a83fd1550000000000000000000080ffffffffffffffa0214858fc7f00000000000000000000
-------bytes.fromhex(unique_id): b'O\x19\x9d\x15\xc9]\x8a5\x02\x00\xc9M\xc0\xa8\xa2\xc3\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00Oi\x01V\xd6\x7f\x00\x00\xd0\xed\xf4\xcc\xd0U\x00\x00\x9c\xe4j\xd7\xd4\x7f\x00\x00\xa0!HX\xfc\x7f\x00\x00 \x00\x00\x00\x00\x00\x00\x00\r\x00\x00\x00\x00\x00\x00\x00`c\xa8?\xd1U\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x80\xff\xff\xff\xff\xff\xff\xff\xa0!HX\xfc\x7f\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00'
NCCL version 2.21.5+cuda11.0
rank 0 config  <bmtrain_paddle.nccl.NCCLCommunicator object at 0x7fd651f2efb0>
====================== Initialization ======================
rank :          0
local_rank :    0
world_size :    2
local_size :    2
master :        localhost:10010
device :        gpu:2
cpus :          [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1
                3, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 2
                4, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 3
                5, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 4
                6, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 5
                7, 58, 59, 60, 61, 62, 63]

Traceback (most recent call last):
  File "/home/jiyiming/code/BMTrain_jym/example/train_by_paddle_multi.py", line 168, in <module>
    main()
  File "/home/jiyiming/code/BMTrain_jym/example/train_by_paddle_multi.py", line 23, in main
    model = GPT(
  File "/home/jiyiming/code/BMTrain_jym/example/models/gpt_paddle_bmt.py", line 42, in __init__
    self.transformers = bmt.TransformerBlockList([
  File "/home/jiyiming/code/BMTrain_jym/example/models/gpt_paddle_bmt.py", line 43, in <listcomp>
    bmt.Block(
  File "/home/jiyiming/.local/lib/python3.10/site-packages/bmtrain_paddle/block_layer.py", line 107, in __init__
    self.init_param_storage()
  File "/home/jiyiming/.local/lib/python3.10/site-packages/bmtrain_paddle/block_layer.py", line 133, in init_param_storage
    kw_name = _get_param_kw(param)
  File "/home/jiyiming/.local/lib/python3.10/site-packages/bmtrain_paddle/block_layer.py", line 50, in _get_param_kw
    if param.group is not None:
AttributeError: 'EagerParamBase' object has no attribute 'group'
I0509 17:17:35.793828 3167673 process_group_nccl.cc:132] ProcessGroupNCCL destruct 
I0509 17:17:35.793877 3167673 process_group_nccl.cc:132] ProcessGroupNCCL destruct 
I0509 17:17:35.793884 3167673 process_group_nccl.cc:132] ProcessGroupNCCL destruct 
I0509 17:17:35.848045 3167785 tcp_store.cc:289] receive shutdown event and so quit from MasterDaemon run loop
