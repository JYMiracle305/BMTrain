-----------  Configuration Arguments -----------
backend: auto
cluster_topo_path: None
coordinator_num: None
coordinators: 
elastic_pre_hook: None
elastic_server: None
enable_auto_mapping: False
force: False
gpus: 2,3
heter_devices: 
heter_worker_num: None
heter_workers: 
host: None
http_port: None
ips: 127.0.0.1
job_id: None
log_dir: log
np: None
nproc_per_node: None
rank_mapping_path: None
run_mode: None
scale: 0
server_num: None
servers: 
training_script: test_allreduce.py
training_script_args: []
worker_num: None
workers: 
------------------------------------------------
launch train in GPU mode!
launch proc_id:3121230 idx:0
launch proc_id:3121232 idx:1
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
=======================================================================
I0509 16:55:31.775357 3121230 tcp_utils.cc:181] The server starts to listen on IP_ANY:45151
I0509 16:55:31.775555 3121230 tcp_utils.cc:130] Successfully connected to 127.0.0.1:45151
I0509 16:55:34.749408 3121230 process_group_nccl.cc:129] ProcessGroupNCCL pg_timeout_ 1800000
FileStore path: ./tmp/bmtrain_store_None_None
FileStore set ./tmp/bmtrain_store_None_None/BMTRAIN_UNIQUE_ID.tmp df2ecea49f21504b0200c873c0a8a2c30000000000000000000000000000000000000000000000004f6921981f7f0000a0b763a6dc5500009ce46ad51e7f0000e0a448f3ff7f000020000000000000000d0000000000000020e5aba7dc55000000000000000000000200000000000000e0a448f3ff7f00000000000000000000
rank 0 unique_id df2ecea49f21504b0200c873c0a8a2c30000000000000000000000000000000000000000000000004f6921981f7f0000a0b763a6dc5500009ce46ad51e7f0000e0a448f3ff7f000020000000000000000d0000000000000020e5aba7dc55000000000000000000000200000000000000e0a448f3ff7f00000000000000000000
-----------------bytes.fromhex(unique_id), world_size, rank b'\xdf.\xce\xa4\x9f!PK\x02\x00\xc8s\xc0\xa8\xa2\xc3\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00Oi!\x98\x1f\x7f\x00\x00\xa0\xb7c\xa6\xdcU\x00\x00\x9c\xe4j\xd5\x1e\x7f\x00\x00\xe0\xa4H\xf3\xff\x7f\x00\x00 \x00\x00\x00\x00\x00\x00\x00\r\x00\x00\x00\x00\x00\x00\x00 \xe5\xab\xa7\xdcU\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\xe0\xa4H\xf3\xff\x7f\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00' 2 0
NCCL version 2.18.3+cuda11.8
W0509 16:55:36.249039 3121230 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W0509 16:55:36.270887 3121230 gpu_resources.cc:164] device: 2, cuDNN Version: 8.7.
/home/jiyiming/code/BMTrain_jym/example/test_allreduce.py:76: DeprecationWarning: This function is deprecated in favor of cupy.from_dlpack
  cp_array = cupy.fromDlpack(dlpack)
src_ptr: 0x7f1e86c04000
dst_ptr: 0x7f1e86c04200
dst_ptr: 139769086493184
input_paddle_ptr: 0x7f1e86c04400
input_paddle_ptr: 139769086493696
src dst Place(gpu:2) Place(gpu:2)
Rank 0 | Before AllReduce:
src: [1.], dst: [0.]
dst.place Place(gpu:2)
Rank 0 | After AllReduce (sum):
src: [1.], dst: [3.]
I0509 16:55:36.486789 3121230 process_group_nccl.cc:132] ProcessGroupNCCL destruct 
I0509 16:55:36.510705 3121291 tcp_store.cc:289] receive shutdown event and so quit from MasterDaemon run loop
